\section{Statistical model and fit}
\label{sec:htoinv_satistical_treatment}

A likelihood model is used in the fit to data, obtaining the \acrlong{sm} expectation values in the signal and \glspl{CR} as well as testing for signals of \higgstoinv. The observed events counts from data in each subcategory and \ptmiss bin are modelled as Poisson-distributed variables around the \acrshort{sm} expectation with a potential contribution from signal (assumed to be zero in the null hypothesis). Expected event counts in the signal region are obtained from simulation in conjunction with transfer factors from the \glspl{CR} and \glspl{SB} for non-multijet and \acrshort{qcd} multijet processes, respectively, as described in Chpt.~\ref{subsec:htoinv_background_est}. Systematic uncertainties associated with the non-multijet processes, discussed in Chpt.~\ref{sec:htoinv_mc_corrections}, are integrated as nuisance parameters within the model. In full, the likelihood model is described as
\begin{equation}
    \likelihood = \likelihood_{\mathrm{SR}} \cdot \likelihood_{\text{\singleMuCr CR}} \cdot \likelihood_{\text{\doubleMuCr CR}} \cdot \likelihood_{\text{\singleEleCr CR}} \cdot \likelihood_{\text{\doubleMuCr CR}} \cdot \likelihood_{\text{\singlePhotonCr CR}}
    \label{eq:likelihood_overall}
\end{equation}

The likelihood in a given region may be written as multiple Poisson likelihoods (where $\mathcal{P}(n | \lambda) \equiv \frac{ e^{-\lambda} \lambda^n }{n!}$). In the signal region,
\begin{equation}
    \begin{aligned}
\likelihood_{\mathrm{SR}} &= \prod_{\mathrm{subcategory} = i} \prod_{\ptmiss = j} \mathcal{P}(n_{\mathrm{obs.}}^{i, j} | n_{\mathrm{pred.}}^{i, j}), \ \text{where}\\
n_{\mathrm{pred.}}^{i, j} &= r \cdot s^{i, j} \cdot \rho_s^{i, j}\\
&+ b_{\lostlepton}^{i, j} \cdot a_{\lostlepton}^{i, j} \cdot \phi_{\lostlepton}^{i, j} \cdot \rho_{\lostlepton}^{i, j}\\
&+ b_{\ztonunu}^{i, j} \cdot a_{\ztonunu}^{i, j} \cdot \phi_{\ztonunu}^{i, j} \cdot \rho_{\ztonunu}^{i, j}\\
&+ b_{\mathrm{QCD}}^{i, j} \cdot \omega_{\mathrm{QCD}}^{i, j}
    \end{aligned}
    \label{eq:likelihood_SR}
\end{equation}

where $r$ is the unconstrained signal strength parameter, $s$ is the signal expectation determined from simulation, $\rho$ encodes the systematic uncertainties associated with simulation, $b$ is the predicted number of events (from Chpt.~\ref{subsec:htoinv_background_est}), $a$ is an unconstrained rate parameter connecting the signal and relevant \glspl{CR}, $\phi$ encodes the systematic uncertainties associated with the transfer factors within $b$, and $\omega$ contains the uncertainties on the \acrshort{qcd} multijet background estimate. For the control regions, the likelihoods are
\begin{equation}
    \begin{aligned}
\likelihood_{\text{\singleMuCr CR}} &= \prod_{i} \prod_{j} \mathcal{P}( n_{\mathrm{obs.}, \, \Pmu }^{i, j} | r \cdot s^{i, j}_{\Pmu} \cdot \rho_{s,\, \Pmu}^{i, j} + b_{\Pmu}^{i, j} \cdot a_{\lostlepton}^{i, j} \cdot \rho_{\Pmu}^{i, j} )\\
\likelihood_{\text{\doubleMuCr CR}} &= \prod_{i} \prod_{j} \mathcal{P}( n_{\mathrm{obs.}, \, \Pmu\Pmu }^{i, j} | r \cdot s^{i, j}_{\Pmu\Pmu} \cdot \rho_{s,\, \Pmu\Pmu}^{i, j} + b_{\Pmu\Pmu}^{i, j} \cdot a_{\ztonunu}^{i, j} \cdot \rho_{\Pmu\Pmu}^{i, j} )\\
\likelihood_{\text{\singleEleCr CR}} &= \prod_{i} \prod_{j} \mathcal{P}( n_{\mathrm{obs.}, \, \Pe }^{i, j} | r \cdot s^{i, j}_{\Pe} \cdot \rho_{s,\, \Pe}^{i, j} + b_{\Pe}^{i, j} \cdot a_{\lostlepton}^{i, j} \cdot \rho_{\Pe}^{i, j} )\\
\likelihood_{\text{\doubleEleCr CR}} &= \prod_{i} \prod_{j} \mathcal{P}( n_{\mathrm{obs.}, \, \Pe\Pe }^{i, j} | r \cdot s^{i, j}_{\Pe\Pe} \cdot \rho_{s,\, \Pe\Pe}^{i, j} + b_{\Pe\Pe}^{i, j} \cdot a_{\ztonunu}^{i, j} \cdot \rho_{\Pe\Pe}^{i, j} )\\
\likelihood_{\text{\singlePhotonCr CR}} &= \prod_{i} \prod_{j} \mathcal{P}( n_{\mathrm{obs.}, \, \Pphoton }^{i, j} | r \cdot s^{i, j}_{\Pphoton} \cdot \rho_{s,\, \Pphoton}^{i, j} + b_{\Pphoton}^{i, j} \cdot a_{\ztonunu}^{i, j} \cdot \rho_{\Pphoton}^{i, j} + b_{\mathrm{QCD}}^{i, j} \cdot \omega_{\mathrm{QCD}}^{i, j} )
    \end{aligned}
    \label{eq:likelihood_CRs}
\end{equation}

where the products over the indices $i$ and $j$ are the same as in Eq.~\ref{eq:likelihood_SR}. The rate parameters $a$ are shared across the relevant \glspl{CR} for the same subcategories and \ptmiss bins, i.e., $a_{\lostlepton}$ in the single lepton regions, and $a_{\ztonunu}$ in the dilepton and single photon regions. \acrshort{qcd} is not included in any of the lepton \glspl{CR}, but is estimated from the photon purity measurement in Chpt.~\ref{subsubsec:htoinv_photon_purity} for the \singlePhotonCr \gls{CR}.

% Give an outline of the how the fit is done (likelihood model, writing it out in full and highlighting each aspect) and how the background estimation methods factor in, etc. Like background estimation, describe it generally since I didn't explicitly work on it. Current fit model is given at https://indico.cern.ch/event/934008/contributions/3924639/attachments/2065231/3465892/2020_06_15_CHIP_Meeting_nonVBF_fit.pdf

% Describe how processes are grouped together in signal region (ttbar/W/residual SM, Z->inv., QCD in signal region, everything but QCD grouped as electroweak in the CRs, QCD dropped in lepton CRs since it has very negligible presence except for unnatural high-weight events, but kept in photon CR using purity measurement)

% In the following sections, talk more about the specific aspects for each mode (e.g., dropping dilepton and photon CRs for ttH, using photon CR only for VH and ggF)

% Toys (verbatim from Henning): toys are repeat experiments where you draw new values for your observed signal and background based on the uncertainties associated with both. The parameters values are randomly drawn according to some pdf. I.e. if we talk about large event counts this would be gaussian with width equal to uncertainty,  small event counts Poissonian, for syst uncert possibly log-normal.

% autoMCstats reference: https://arxiv.org/pdf/1103.0354.pdf


%=========================================================


\subsection{Binning}
\label{subsec:htoinv_binning}

In addition to the categories in Chpt.~\ref{sec:htoinv_categorisation}, events are further separated into bins of \ptmiss as that distribution is expected to maximally differentiate signal and background in the fit. Since the number of events can significantly differ between categories in the different regions of the analysis, a global binning configuration is inadequate. For each category, the number and widths of the bins are tuned to ensure sufficient statistical precision. These schemes are tied to the category, so are reflected in all regions such that transfer factors from the background estimation methods can be calculated on a bin-by-bin basis unless stated otherwise.\footnote{Should I have a table or something that shows the binning scheme for each subcategory, similar to RA1?}


%=========================================================


\subsection{Background estimation}
\label{subsec:htoinv_background_est}

% Here, give an overview of how the background estimation is done. Just say what the procedure is, generally, for each background

Accurate estimation of the \acrlong{sm} background processes in the signal region is paramount to a search for new physics. Mismeasured backgrounds and uncertainties can wash out traces of signal and affect the fit to data. The yields of the minor backgrounds are taken directly from simulation.\footnote{I guess this isn't really true if we group all backgrounds into Ttw and Zinv. In which case, will have to explain the grouping and reword stuff in this paragraph. So it may make sense to describe the fit before this, or integrate the background estimation section into it.} However, the lost lepton, invisibly decaying \PZ boson, and \acrshort{qcd} multijet processes must be predicted more carefully. The single lepton \glspl{CR} are used to constrain the lost lepton background, arising primarily from \ttbarpjets and \wtolnupjets. The dilepton and single photon \glspl{CR} predict the \ztonunupjets background. These processes rely on the \acrlong{mc} yields in those regions, and transfer factors arising from the \acrshort{mc} ratios between them and the signal region. Sidebands to the signal region estimate \acrshort{qcd} multijet contributions from data. The predicted background yields from each of these methods replace the \acrlong{mc} in the signal region when fed into the fit in Chpt.~\ref{sec:htoinv_satistical_treatment}. Fig.~\ref{fig:htoinv_fit_overview} illustrates the correspondence between the analysis regions and background predictions.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/fit_overview.pdf}
    \caption[An infographic showcasing the role of each analysis region in the final fit]{An infographic showcasing the role of each analysis region in the final fit. The \glspl{CR} predict the lost lepton and \ztonunu backgrounds, and a \gls{CR}-only fit informs the \acrshort{qcd} multijet prediction that contributes to the eventual background determination in the signal region.}
    \label{fig:htoinv_fit_overview}
\end{figure}


%=========================================================


\subsubsection{Lost lepton (\texorpdfstring{\PW}{W} and \texorpdfstring{$\ttbarpjets$}{ttbar plus jets})}
\label{subsubsec:htoinv_lost_lepton_bkg}

In order to predict the lost lepton background in the signal region, a transfer factor $\transfac$ from simulation is defined that then scales the observed data in the single lepton \glspl{CR}. Events are categorised and binned in the same manner in the single lepton \glspl{CR} as in the signal region, such that the prediction is derived bin-by-bin. The transfer factor is simply the ratio of the simulation in the signal region to the \gls{CR}:
\begin{equation}
    \TF_{\singleMuCr} = \frac{ N_{\mathrm{MC}}^{\lostlepton, \, \mathrm{SR}}(\mathrm{subcategory}, \, \ptmiss) }{ N_{\mathrm{MC}}^{\singleMuCr}(\mathrm{subcategory}, \, \ptmiss) }
\end{equation}

The predicted yield in the signal region is then
\begin{equation}
    N_{\mathrm{pred.}}^{\lostlepton}(\mathrm{subcategory}, \, \ptmiss) = \TF_{\singleMuCr} \cdot N_{\mathrm{data}}^{\singleMuCr}(\mathrm{subcategory}, \, \ptmiss)
\end{equation}

% How is this modified when electrons are used as well?


%=========================================================


\subsubsection{\texorpdfstring{\ztonunupjets}{Z to nunu + jets}}
\label{subsubsec:htoinv_znunu_bkg}


%=========================================================


\subsubsection{QCD multijet}
\label{subsubsec:htoinv_qcd_multijet_bkg}

The effects of \gls{jet} mismeasurements are difficult to quantify. With a final state of several \glspl{jet} in the \acrshort{qcd} multijet process, low, or even no, \ptvecmiss is expected. Therefore, a single mismeasured \gls{jet} will introduce artificial \ptvecmiss in the direction of that jet. A low $\mindphiAB{\mathrm{j}}{\ptvecmiss}$ is therefore expected. Though it is not just this process that suffers---\glspl{jet} from ``cleaner'' processes may also be affected---those with real \ptmiss in an event (e.g., $\ztonunupjets$) are unlikely to be significantly affected by one stray object. The enormous cross section of \acrshort{qcd} multijet also amplifies the problem, making the process as a whole more sensitive to, e.g., fluctuations in the calorimeter response that would affect the energy measurement.

Contributions to the signal region from \acrshort{qcd} multijet events should be adequately suppressed by the analysis-level selection requirements. However, it is still a process that must be accurately accounted for considering its rate of production at \acrshort{cms}. A metric by which to estimate the number of events a dataset should require is by calculating the equivalent luminosity:
\begin{equation}
    \lumi_{\mathrm{eq.}} = \frac{N_{\mathrm{events}}}{\sigma}
    \label{eq:equivalent_lumi}
\end{equation}

A general rule is that the equivalent luminosity of a given dataset should be comparable to, or even exceed, that of the data collected by the experiment. Since the \acrshort{qcd} multijet process has a very large cross section, simulating the required number of events to match the luminosity of the data recorded during Run-2 is not feasible. This can be mitigated by using a data-driven method to estimate it from the multijet-enriched sidebands described in Chpt.~\ref{subsec:htoinv_sidebands}.

To estimate the presence of \acrshort{qcd} in the signal region, a data driven approach is taken utilising the sidebands defined in Chpt.~\ref{subsec:htoinv_sidebands}. These are derived separately for each category and data taking year. Firstly, a fit to the lepton and photon \glspl{CR} is performed to extract the rate parameters that scale the non-multijet background in each subcategory and \ptmiss bin. Applying these scale factors to the relevant backgrounds in the sidebands changes the distribution and hence the data/\acrshort{mc} agreement. The excess in data in a sideband is assumed to arise solely from multijet events, and as such the difference between data and the non-multijet background is attributed to \acrshort{qcd} (denoted as $N_{\mathrm{SB}}^{\mathrm{QCD}}$).

\acrshort{qcd} in the signal region $N_{\mathrm{pred.}}^{\mathrm{QCD}}$ is predicted in each subcategory and \ptmiss bin as follows: 
\begin{equation}
    N_{\mathrm{pred.}}^{\mathrm{QCD}}(\mathrm{subcategory}, \, \ptmiss) = N_{\mathrm{SB}}^{\mathrm{QCD}} \cdot \transfac_{\mathrm{QCD}} \cdot \catFraction(\mathrm{subcategory}) \cdot \metFraction(\ptmiss)
    \label{eq:qcd_prediction}
\end{equation}

% Explaining the TF sounds a bit weird, but will hopefully make more sense once I've explained the transfer factors for the lost lepton and Z->invisible prediction as they're bin-by-bin
where $\transfac_{\mathrm{QCD}}$ is the transfer factor relating the \acrshort{qcd} in the sideband to the multijet simulation in the signal region. As the signal region is depleted in simulated multijet events, the transfer factor is inclusive over the region rather than per \ptmiss bin and subcategory, i.e.,
\begin{equation}
    \transfac_{\mathrm{QCD}} = \frac{ N_{\mathrm{MC, \ SR}}^{\mathrm{QCD}} } { N_{\mathrm{MC, \ SB}}^{\mathrm{QCD}} }
%   \transfac_{\mathrm{QCD}} = \frac{ \sum_{\mathrm{subcategory}, \, \ptmiss}^{\mathrm{SR}} N_{\mathrm{MC}}^{\mathrm{QCD}}(\mathrm{subcategory}, \, \ptmiss) }{ \sum_{\mathrm{subcategory}, \, \ptmiss}^{\mathrm{SB}} N_{\mathrm{MC}}^{\mathrm{QCD}}(\mathrm{subcategory}, \, \ptmiss) }
    \label{eq:transfer_factor_qcd}
\end{equation}

The distribution of the \acrshort{qcd} background for each subcategory and \ptmiss bin is extrapolated from the sidebands with the factors $\catFraction$ and $\metFraction$. $\catFraction$ is the fraction of \acrshort{qcd} in a given subcategory of a given sideband, inclusive of \ptmiss. For the \ttH and \ggH categories, the tight double sideband from Tab.~\ref{tab:sideband_defs}---the most enriched in \acrshort{qcd} \acrshort{mc}---is used to derive the factor. $\metFraction$ is the fraction of \acrshort{qcd} in a given \ptmiss bin of a given sideband, inclusive of subcategory. For the \ttH and \ggH categories, the loose double sideband is used to derive the factor. This sideband is used for the prediction as a whole, i.e., for the terms $N_{\mathrm{SB}}^{\mathrm{QCD}}$ and $\transfac_{\mathrm{QCD}}$ in Eq.~\ref{eq:qcd_prediction}.

% Need to mention how prediction is done for VH, given sidebands 0 and 2 are pretty depleted in stats. Also need to mention how uncertainty is handled for the predicted QCD
