\section{Overview of the analysis}
\label{sec:htoinv_analysis_overview}

The analysis discussed in the remainder of the chapter is a dedicated search for invisibly decaying Higgs bosons in hadronic final states, incorporating three of the four main production modes. In contrast to many of the previous analyses that separately set limits on $\BRof{\higgstoinv}$ by reinterpretation, this approach has many benefits. A simultaneous search for several production modes allows the construction of search regions to target each one, embedding orthogonality to avoid overlap between them. Data and simulation samples, recipes for corrections and systematic uncertainties, the analysis framework, and results can all be shared to provide a cohesive and consistent environment from which to perform the analysis. As well as streamlining the process of the final combination over each production mechanism, communication when establishing the analysis ensures each can cover as much phase space as possible without the trouble of overlap or contamination.

Direct collaboration between the University of Bristol and Imperial College London divides up the task of analysing the various Higgs boson production modes appropriately. This thesis focuses on \ttH, \VH and \ggH mechanisms. Those at Imperial College London, who have a long history with the \acrshort{vbf} search~\cite{Chatrchyan:2014tja,Sirunyan:2018owy}, assume responsibility of this process. Results will be shared between the institutes after the publication of this document to achieve a fully combined limit on $\BRof{\higgstoinv}$.


%=========================================================


\subsection{Analysis strategy}
\label{subsec:htoinv_analysis_strategy}

Emphasis is placed on \ttH and \VH as they are novel dedicated searches. The \ggH mechanism also under our charge, and accordingly will be given attention. Given the \acrshort{lhc} is a \emph{hadron} collider, hadronic final states are naturally chosen for this analysis. \Gls{met} from the purely-invisible decay of the Higgs---and hadronic activity from the particles associated with the production mechanism---constitute the final state, often known as a ``\text{\glspl{jet}} $+$ \ptvecmiss'' search. In lieu of this, the dominant background processes from the \acrlong{sm} include \acrshort{qcd} with high \gls{jet} multiplicity, invisible decays of the \PZ boson, and those where the leptons from the decay are ``lost'' (\lostlepton) from misidentification or are outside the bounds of the detector acceptance. The latter is predominantly populated by \ttbar and leptonically decaying \PW bosons. To accurately estimate their presence in the signal region of the analysis, dedicated methods are employed. \Glspl{CR} separated by lepton and photon multiplicity predict the lost lepton and \ztonunu processes. Sidebands to the signal region, where one or two selections otherwise designed to reject \acrshort{qcd} multijet events are inverted, give rise to phase spaces enriched in them. A data-driven approach utilises these to predict the multijet background in the signal region.

Definitions of the physics objects used analysis-wide have already been discussed in Chpt.~\ref{sec:analysis_objects}. The data from \acrshort{cms} and from simulation are reported in Chpt.~\ref{sec:htoinv_data_sim}. In addition, corrections to simulation are outlined which are designed to describe the data more accurately. These are followed by the event selection in Chpt.~\ref{sec:htoinv_event_selection} to separate signal from background and discard poorly measured events. Categorisation of the remaining events to highlight the production modes is illustrated in Chpts.~\ref{sec:htoinv_categorisation}. These are further separated into the various phase space regions in Chpt.~\ref{sec:region_definitions}. Systematic uncertainties related to simulation and its corrections are discussed in Chpt.~\ref{sec:htoinv_mc_corrections}. The culmination of all of the aforementioned sections is in the fit of signal and background simultaneously in the signal and \glspl{CR} to data in Chpt.~\ref{sec:htoinv_satistical_treatment}. Estimation of the dominant background processes from the \glspl{CR} and \glspl{SB}---and how they are incorporated into the fit---are as well documented. Results of the analysis of the \ttH, \VH, and \ggH modes are illustrated in Chpts.~\ref{sec:htoinv_analysis_ttH}, \ref{sec:htoinv_analysis_VH} and \ref{sec:htoinv_analysis_ggF}, respectively. A combination of all of the production modes and data taking years to yield the final result close the chapter in Chpt.~\ref{sec:htoinv_combined_results} along with interpretations in simplified dark matter scenarios.


%=========================================================


\section{Only tools and forces: software and toolkits}
\label{sec:htoinv_software}

The high energy physics community is well-acquainted with the software package \ROOT. The extensive suite of functions and tools it offers makes it the de facto standard in many analyses. Even though it is entrenched in particle physics, the native C\texttt{++} implementation and difficulty in integrating it with Python make \ROOT often cumbersome for newcomers to learn, and can be difficult to develop an analysis around it. In recent years, focus has started to shift toward industry-standard Python libraries such as \textsf{numpy} and \textsf{pandas} for analysis, and \textsf{matplotlib} or \textsf{seaborn} for plotting.

The FAST set of tools utilise these, along with vectorisation, to perform an analysis very quickly with simple YAML-controlled config files. Jobs can be submitted on the batch system at a given site, running the full analysis on the upwards of 15 billion events in as little as thirty minutes. Developed by colleagues at Bristol, a set of packages harmoniously work together to run most components of an analysis. Dataset information is gathered with \textsf{fast-curator} and stored in YAML configs. With \textsf{fast-carpenter}, modules (both custom and internal) for the different stages of the analysis process the datasets. Output is in the form of binned \textsf{pandas} dataframes of user-specified variables. They may be subsequently be fed into \textsf{fast-plotter} for visualisation as 1D histograms. All of the stages are controlled by YAML configs and are interpreted by \textsf{fast-flow}. Only a simple interface to \ROOT is required to extract the data. Further information on these packages are available at \url{http://fast-hep.web.cern.ch/fast-hep/}.

Before the main analysis is run with the above tools, a light skim---or reduction of events---of the remotely-available datasets is effectuated with \nanoAODtools.\footnote{See \url{https://github.com/cms-nanoAOD/nanoAOD-tools} for the original fork of the repository.} Operating on the nanoAOD data tier, the repository contains corrections, scale factors, and systematic uncertainties for \acrshort{cms} data and simulation. They are maintained by members of the Collaboration, so it is ensured they remain up to date. Our custom modules are applied on top of this: introducing additional scale factors and systematic uncertainties, classification of some physics objects, and calculating complex variables. Processing the datasets on the Worldwide LHC Computing Grid, output is stored on networked university storage elements. The primary benefit of this is a quicker input-output stream that improves the performance of the later stages of the analysis.
