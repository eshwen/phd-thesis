\section{Only tools and forces: software and toolkits}
\label{sec:htoinv_software}

Analysing high energy physics data is a long and complex task with many stages that must be stitched together. In this analysis, the first step involves a light skim---or reduction of events---of the remotely-available datasets, and is effectuated with \nanoAODtools.\footnote{See \url{https://github.com/cms-nanoAOD/nanoAOD-tools} for the original fork of the repository.} Operating on the nanoAOD data tier, the repository contains centrally-maintained corrections and systematic uncertainties related to physics objects. Custom modules are applied on top. Processing the datasets on the Worldwide LHC Computing Grid, output is stored on networked university storage elements for improved performance of the later stages of the analysis.

Skimmed data is analysed predominantly using the FAST set of tools~\cite{fast_hep_epj}, developed by colleagues at the University of Bristol. A suite of packages harmoniously work together to run most components of an analysis. The event selection and categorisation, as well as many studies and measurements of the data are conducted at this stage. Use of vectorisation and industry-standard Python libraries such as \textsf{numpy} and \textsf{pandas} allow complex and efficient processing with simple syntax. Visualisation can also be achieved with interfaces to \textsf{matplotlib}.

Output from the previous stage is processed through the fit of signal and background to data. Specification of all aspects are handled with the \textsf{HiggsAnalysis-CombinedLimit} package.\footnote{Official documentation: \url{https://cms-analysis.github.io/HiggsAnalysis-CombinedLimit/}.} A plethora of diagnostic information is available for understanding the effects of systematic uncertainties and other aspects of the analysis on the results.

%=========================================================
