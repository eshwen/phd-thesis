\section{Corrections and systematic uncertainties for simulation}
\label{sec:htoinv_mc_corrections}

% Weights/SFs and systematics in the analysis I need to still describe:
%   - JES/JER

In order for simulated events, particularly for background processes, to resemble \acrshort{lhc} data as closely as possible, many corrections and weights are applied. These are discussed in more detail in the subsequent sections. All weights and associated systematic uncertainties are applied to all samples for all data taking years, unless stated otherwise. They are summarised in Tab.~\ref{tab:htoinv_syst_summary}. A final event weight $w_{\mathrm{event}}$ is the product of the weights from all of the individual sources $i$ that provide one:
\begin{equation}
    w_{\mathrm{event}} = \prod_i w_i
    \label{eq:event_weight}
\end{equation}

When representing these events in histograms, the yield in a given bin $N_{\mathrm{corr.}}$ is the sum of these event weights:
\begin{equation}
    N_{\mathrm{corr.}} = \sum_j^{N_{\mathrm{MC}}} w_{\mathrm{event} \ j}
    \label{eq:bin_weight}
\end{equation}

where $N_{\mathrm{MC}}$ is the number of unweighted, simulated events in the bin. In the data-\acrshort{mc} distributions, the statistical uncertainty ascribed to the \acrshort{mc} yield in a bin is estimated from the unweighted number of events ($k$ in a Poisson statistical treatment):
\begin{equation}
    \Delta N_{\mathrm{corr.}} = \pm \frac{ N_{\mathrm{corr.}} }{ \sqrt{N_{\mathrm{MC}}} }
    \label{eq:uncertainty_mc_ours}
\end{equation}

While the statistical uncertainty for the number of events in data is simply the Poissonian error,
\begin{equation}
    \Delta N_{\mathrm{data}} = \pm \frac{ N_{\mathrm{data}} }{ \sqrt{N_{\mathrm{data}}} }
    \label{eq:uncertainty_data}
\end{equation}


%=========================================================


\subsection{Efficiency of the triggers}
\label{subsec:htoinv_trigger_effs}

Given the modelling of the \acrlong{l1} and \acrlong{hlt} systems in simulation is only an approximation of their behaviour with data, corrections must be made to unite them.

A dedicated \gls{CR} is formed to assure orthogonality with the signal region and avoid unblinding of data, but with a similar kinematic selection to the analysis for accuracy in our phase space. Events are selected in data from the primary dataset consisting of muon-based triggers, and in simulation from the \wtolnupjets process at \acrshort{lo}. Analysed events consist of these containing a single, tightly-isolated muon with $\pt > \text{30}\GeV$ that triggers the \acrshort{hlt} path for an isolated muon with $\pt > \text{27}\GeV$.\footnote{Why exactly do we use single muon events. Is it the high efficiency from the muon subsystem? Or that they won't bias the METMHT trigger efficiencies, or something?} An offline selection is then applied to keep those with similar kinematic properties to those in our signal region:

\medskip
\begin{easylist}[itemize]
    \cutflowlistprops
    & $\ptjone > \text{80}\GeV$
    & $\HT > \text{200}\GeV$
    & $\mht/\ptmissNoMu < \text{1.2}$
\end{easylist}

\medskip

\noindent{}where $\ptmissNoMu = \ptmiss + \pt^{\Pmu}$ to approximate the \acrshort{pf} calculation at \acrshort{hlt} level. Of these, events that additionally pass the online selection of the trigger requirements in the signal region (Tab.~\ref{tab:htoinv_SR_triggers}) are also recorded. The efficiency of a trigger---or collection of triggers---$\epsilon_{\mathrm{trg}}$ is defined as
\begin{equation}
    \epsilon_{\mathrm{trg}} = \frac{ s_\mathrm{offline} \cap s_{\mathrm{online}} }{ s_\mathrm{offline} }
    \label{eq:trigger_eff}
\end{equation}

where $s$ denotes the set of events that have passed the selection given by its subscript. For simulation, events are weighted by cross section as per Eq.~\ref{eq:xs_weight}. The efficiencies are binned in two dimensions as a function of \ptmissNoMu and \mht. The weight applied to events in simulation $w_{\mathrm{trg}}$ is then simply the efficiency in data divided by the efficiency in simulation for the values of \ptmiss and \mht in the event:
\begin{equation}
    w_{\mathrm{trg}}(\ptmiss, \mht) = \frac{\epsilon_{\mathrm{trg, \ data}}(\ptmiss, \mht)}{\epsilon_{\mathrm{trg, \ MC}}(\ptmiss, \mht)}
    \label{eq:trigger_weight}
\end{equation}

Uncertainties are estimated using the Clopper-Pearson method~\cite{10.1093/biomet/26.4.404}. In the signal region, sidebands, and muon \glspl{CR}, weights from the efficiencies of the \ptmiss--\mht cross triggers are used. The weights are provided for each data taking year in Fig.~\ref{fig:htoinv_trig_effs}. In the electron and photon \glspl{CR}, the efficiencies are calculated in the same manner, instead from the triggers in Tabs.~\ref{tab:htoinv_ele_pd_triggers} and \ref{tab:htoinv_photon_pd_triggers}.

\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \includegraphics[width=\textwidth]{figures/trigger_efficiencies/2016/SFs.pdf}
        \caption{2016}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \includegraphics[width=\textwidth]{figures/trigger_efficiencies/2016/SFs.pdf}
        \caption{2017}
    \end{subfigure}

    \begin{subfigure}[b]{0.48\textwidth}
        \includegraphics[width=\textwidth]{figures/trigger_efficiencies/2017/SFs.pdf}
        \caption{2018}
    \end{subfigure}
    \caption[Scale factors accounting for the efficiencies of the HLT \ptmiss--\mht cross triggers in each data taking year of Run-2]{Scale factors accounting for the efficiencies of the \acrshort{hlt} \ptmiss--\mht cross triggers in each data taking year of Run-2. The momentum of the muon used to select events that compute the efficiency is added to the \ptvecmiss (with a magnitude \ptmissNoMu) in order to approximate the calculation in the \acrlong{hlt}.}
    \label{fig:htoinv_trig_effs}
\end{figure}


%=========================================================


\subsection{Pileup reweighting}
\label{subsec:pu_reweighting}

\Gls{pileup} interactions at the \acrshort{lhc} are frequent (see Chpt.~\ref{subsec:pileup}) and must be modelled appropriately in simulation. Simulated samples are generated with a certain distribution of the number of \gls{pileup} interactions which usually does not match the data recorded by \acrshort{cms}. This is due to changing conditions in the beam over a period of data taking. In order to make them comparable, the simulated events are reweighted; in this context it is known as \emph{\gls{pileup} reweighting}. \ROOT files containing histograms of the number of \gls{pileup} interactions from short runs in the \acrshort{lhc} are available centrally and are used as the reference for which to reweight the simulated events.

Simulated events are nominally reweighted according to data, where the inelastic $\pp$ cross section is measured to be 69.2\,mb. An uncertainty of $\pm \text{4.6}$\,\% in the measurement is used to calculate the systematic uncertainty on this weight. The pileup distributions, and therefore the weights, are different for each data taking year. However, the inelastic cross section and associated uncertainty are consistent for the entirety of Run-2.

% https://github.com/cms-nanoAOD/nanoAOD-tools/blob/048a828e1d7f5ac3346e2f5e7eafba2570e84bc4/python/postprocessing/modules/common/puWeightProducer.py for implementation


%=========================================================


\subsection{Higher order corrections to \texorpdfstring{$\PVec \plusjets$}{V plus jets} samples}
\label{subsec:htoinv_nlo_corrs}

Vector boson \plusjets processes present a sizeable background in some of the categories and regions of the analysis. At the dataset sizes required to provide sufficient statistical coverage of the phase space, \acrshort{nlo} \acrshort{mc} samples are not available with the complete detector simulation applied. To circumvent this, \acrshort{lo} samples are used in the analysis and reweighted at generator level on an event-by-event basis to \acrshort{nlo} accuracy. This method is usually referred to as a ``$k$-factor'' correction, where $k$ is the ratio of the \acrshort{nlo} to \acrshort{lo} cross section.

Separate \acrshort{qcd} and electroweak corrections are applied to \acrshort{qcd} processes. \acrshort{nlo} $\PW \plusjets$ samples are used to derive the $k$-factors for the respective \acrshort{lo} process. Drell-Yan $\HepProcess{\PZ (\to \Plepton\APlepton)} \plusjets$ samples are utilised for both the \acrshort{lo} Drell-Yan and \ztonunupjets processes. The $k$-factors are calculated independently for the two processes as the \acrshort{lo} samples and cross sections are different. One or two additional \glspl{jet} in the matrix element calculations are permitted in the \acrshort{nlo} samples. Events from the above datasets are processed through the following generator-level event selection to ensure they are in a similar phase space to the analysis:

\medskip
\begin{easylist}[itemize]
    \cutflowlistprops
    & $\ptjone > \text{80}\GeV$
    & $\pt^{\PVec} > \text{200}\GeV$
    & $\HT^{\mathrm{Gen}} > \text{200}\GeV$
    & $\HT^{\mathrm{Gen}}/\pt^{\PVec} < \text{1.2}$\footnote{Not sure if we'll need updated k-factors if we switch to another scenario where we don't use the MHT/MET cut.}
\end{easylist}

\medskip

\noindent{}where $\pt^{\PVec}$ is the generator-level boson \pt, and $\HT^{\mathrm{Gen}}$ is the scalar sum of generator-level \gls{jet} \pt. The final selection in the list mimics the $\mht/\ptmiss$ cut in Chpt.~\ref{subsec:htoinv_preselection}. The $k$-factors are binned in two dimensions, $\pt^{\PVec}$ and $\ptjone$. Uncertainties for the \acrshort{qcd} renormalisation scale, factorisation scale, and in the parton distribution function are treated as individual, uncorrelated sources. The same values for the nominal $k$-factors and uncertainties are used for each data taking year in Run-2. Distributions of the \acrshort{nlo} \acrshort{qcd} $k$-factors are presented in Fig.~\ref{fig:htoinv_nlo_k_factors_qcd}.

\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/nlo_k_factors/2D_zll.pdf}
        \caption{Drell-Yan $\HepProcess{\PZ \to \Plepton\APlepton}\plusjets$}
    \end{subfigure}
    \hspace{0.05\textwidth}
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/nlo_k_factors/2D_znunu.pdf}
        \caption{$\ztonunupjets$}
    \end{subfigure}

    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/nlo_k_factors/2D_wjets.pdf}
        \caption{$\wtolnupjets$}
    \end{subfigure}
    \caption[NLO $k$-factors---as a function of the generator-level boson \pt and leading jet \pt---used to reweight events in LO QCD $\PW$ and $\PZ \plusjets$ simulation to NLO accuracy in QCD]{\acrshort{nlo} $k$-factors---as a function of the generator-level boson \pt and leading jet \pt---used to reweight events in \acrshort{lo} \acrshort{qcd} $\PW$ and $\PZ \plusjets$ simulation to \acrshort{nlo} accuracy in \acrshort{qcd}.}
    \label{fig:htoinv_nlo_k_factors_qcd}
\end{figure}

Since all of the $\PVec \plusjets$ samples were generated at \acrshort{lo} electroweak accuracy, an additional $k$-factor is needed to reweight events to \acrshort{nlo}. Ref.~\citenum{Lindert_2017}{}provides \acrshort{nlo} electroweak corrections for \PW, \PZ, and $\Pphoton\plusjets$ processes as a function of $\pt^{\PVec}$, depicted in Fig.~\ref{fig:htoinv_nlo_k_factor_ewk}. The \PZ $k$-factor is adopted by both the Drell-Yan and \ztonunu processes.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.45\textwidth]{figures/nlo_k_factors/1D_all_ewk.pdf}
    \caption[NLO $k$-factors as a function of generator-level boson \pt used to reweight QCD $\PVec \plusjets$ events generated at LO electroweak accuracy]{\acrshort{nlo} $k$-factors as a function of generator-level boson \pt used to reweight \acrshort{qcd} $\PVec \plusjets$ events generated at \acrshort{nlo} electroweak accuracy. Separate $k$-factors correct the \PW, \PZ, and $\Pphoton\plusjets$ processes.}
    \label{fig:htoinv_nlo_k_factor_ewk}
\end{figure}


%=========================================================


\subsection{Top quark processes}
\label{subsec:htoinv_ttbar_uncerts}

\ttbarpjets is the dominant background for the \ttH category. The $\ttbar X$ backgrounds also have a non-negligible presence. As such, much effort ensures they are modelled correctly. Variations of the \acrshort{qcd} renormalisation and factorisation scales provide an uncertainty only to the shape of the \ptmiss spectrum of \ttbar and $\ttbar X$ backgrounds, and \ttH signal. Reweighting of the top quark \pt distribution to \acrshort{nnlo} accuracy is also performed for \ttbar, yielding a shape correction and associated uncertainty.


%=========================================================


\subsubsection{QCD scale systematic uncertainty}
\label{subsubsec:ttbar_renorm_fact_scale_uncert}

The renormalisation \muR and factorisation scale \muF chosen for \acrshort{mc} samples at generation may be somewhat arbitrary, and therefore not representative of nature. Relative to the nominal, weights were derived centrally for each combination of these scales being varied independently (and together) up or down by a factor of two. Eight variations then exist for each event:
\medskip
\begin{easylist}[itemize]
    \easylistprops
    & $\muRAmuFB{\downarrow}{\downarrow}$
    & $\muRAmuFB{\downarrow}{\, \mathrm{nom.}}$
    & $\muRAmuFB{\downarrow}{ \uparrow}$
    & $\muRAmuFB{\, \mathrm{nom.}}{\downarrow}$
    & $\muRAmuFB{\, \mathrm{nom.}}{ \uparrow}$
    & $\muRAmuFB{\uparrow}{\downarrow}$
    & $\muRAmuFB{\uparrow}{\, \mathrm{nom.}}$
    & $\muRAmuFB{\uparrow}{\uparrow}$
\end{easylist}

\medskip

\noindent{}To remove the dependence on normalisation (since the skim and event selection may bias it) and ensure the systematic is only shape-based, each variation was also multiplied by an additional factor $f_{\mathrm{var}}$:
\begin{equation}
    f_{\mathrm{var}} = \dfrac{ \sum_{\text{unskimmed events}} w_{\muRAmuFB{\, \mathrm{nom.}}{\, \mathrm{nom.}}} }{ \sum_{\text{unskimmed events}} w_{\mathrm{var}} }
    \label{eq:ttbar_scale_fvar}
\end{equation}

These are computed individually for each decay and data taking year. The envelope is characterised by the correlated variations $\muRAmuFB{\uparrow}{\uparrow}$, and $\muRAmuFB{\downarrow}{\downarrow}$. Confirmed by Fig.~\ref{fig:htoinv_ttbar_scale}, an example is given for the \ttbar background in the signal region. While the sizes of the variations appear large, they are very similar between the signal and \glspl{CR}, cancelling to a large degree to minimally impact the results.

\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{0.34\textwidth}
        \includegraphics[width=\textwidth]{figures/qcd_scale_top_procs/ttbar/ratio_vars_SR_ttH_boosted.pdf}
        \caption{\ttH boosted}
    \end{subfigure}
    \hspace{0.05\textwidth}
    \begin{subfigure}[b]{0.34\textwidth}
        \includegraphics[width=\textwidth]{figures/qcd_scale_top_procs/ttbar/ratio_vars_SR_ttH_resolved.pdf}
        \caption{\ttH resolved}
    \end{subfigure}
    \caption[The deviations relative to the nominal weight of the combinations of QCD renormalisation and factorisation scale variations. They are presented as a function of \ptmiss in the \ttH categories for the \ttbar samples]{The deviations relative to the nominal weight of the combinations of \acrshort{qcd} renormalisation and factorisation scale variations. They are presented as a function of \ptmiss in the \ttH categories for the \ttbar samples. Events are from the signal region in the 2017 dataset after the analysis-level selection.}
    \label{fig:htoinv_ttbar_scale}
\end{figure}

% Plots from Scenario 5, 8th July 2020

For each process (i.e., \ttbar, and the accompanying boson $X$ in $\ttbar X$), a separate systematic uncertainty is derived.\footnote{$\ttH(\HepProcess{\PH \to \text{invisible}})$ signal and $\ttH(\HepProcess{\PH \to \text{visible}})$ background are grouped as one process since the kinematics and generation procedure are consistent.} Since different generators and settings may be used for each dataset, it is appropriate to ensure they are uncorrelated.


%=========================================================


\subsubsection{Top quark \texorpdfstring{\pt}{pT} reweighting}
\label{subsubsec:htoinv_top_pt_reweighting}

The top quark \pt distribution has been shown to be harder in simulation at \acrshort{nlo} than in data~\cite{Sirunyan:2018ucr}. \acrshort{nlo} \ttbarpjets \acrshort{mc} generated in \POWHEG is reweighted to \acrshort{nnlo} \acrshort{qcd} $+$ \acrshort{nlo} electroweak accuracy as function of top quark \pt. An analytic function is used to fit the distribution that determines the weight. The same nominal function is used for all \ttbarpjets samples for all data taking years:
\begin{equation}
    f(\pt) = \exp(a + b \cdot \pt + c \cdot \pt^2)
\end{equation}

where the coefficients are $a = \text{1.614} \times \text{10}^{-2}$, $b = -\text{1.966} \times \text{10}^{-4}$, and $c -\text{1.454} \times \text{10}^{-8}$. For a top or antitop quark, the scale factor is given by the value of $f$ for the particle's generator-level \pt. The event weight is then the square root of the product of the top and antitop scale factors.

The uncertainties are acquired from the fit itself over theoretical uncertainties inherent in \acrshort{mc} calculations. This was to avoid double counting with respect to the renormalisation and factorisation scale uncertainty in Chpt.~\ref{subsubsec:ttbar_renorm_fact_scale_uncert}, as overlap would be present and otherwise difficult to disentangle. For each parameter in the fit, the upward and downward variations were estimated. The fit function was then recalculated, fixing the parameter of interest to the variation. Providing upward and downward functions, the systematic uncertainty attributed to the parameter is simply the event weight calculated with these functions over the nominal. Separate, uncorrelated uncertainties were estimated for each parameter in the fit. The nominal function and associated variations are shown in Fig.~\ref{fig:top_pt_reweighting}.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/top_pt_reweighting/top_pt_reweighting_central.pdf}
    \caption[The ratio of the NNLO to NLO \ttbar cross section as a function of top quark \pt. The fit to the distribution---along with the uncertainties in the parameters---are shown]{The ratio of the \acrshort{nnlo} to \acrshort{nlo} \ttbar cross section as a function of top quark \pt. The fit to the distribution---along with the uncertainties in the parameters---are shown.}
    \label{fig:top_pt_reweighting}
\end{figure}

We aimed to remove the dependence on normalisation in the same fashion as for the \acrshort{qcd} scale to consider the correction only to the shape of the distributions and not also the normalisation. However, the normalisation factors were found to be unity for all datasets. Hence, no additional factor was required.

% See https://indico.cern.ch/event/904971/contributions/3857701/attachments/2036949/3410728/TopPt_20.05.12.pdf for more info


%=========================================================


\subsection{Object-level scale factors and uncertainties}
\label{subsec:htoinv_SFs_systs_objects}

There are multiple corrections applied to simulation based on the physics objects with an event as opposed to the event topology as a whole. One example of which is Chpt.~\ref{subsubsec:htoinv_top_pt_reweighting}. A weight or efficiency $\epsilon$ attributed to an object itself is typically referred to as a \emph{scale factor}, while the event-level selection weight $w_{\mathrm{sel.}}$ is the efficiency for successfully tagging or selecting an event with those object contents, and is the product of the scale factors:
\begin{equation}
    w_{\mathrm{sel.}} = \prod_i^{N_\mathrm{objects}} \epsilon_i
    \label{eq:event_selection_weight}
\end{equation}

These weights are calculated individually for each class of object in an event. For electrons, muons, and photons, these scale factors are usually from the reconstruction efficiency, identification efficiency, and \pt- or $\eta$-dependent energy corrections. In the case of \Pbottom-tagged \glspl{jet}, it is the data--simulation scale factor at the given working point from the algorithm used to identify them. Systematic uncertainties associated with the scale factors are aggregated in the same manner. Unless stated otherwise, the following scale factors and systematics are derived separately for each year in Run-2.


%=========================================================


\subsubsection{Lepton and photon identification, isolation, and reconstruction}
\label{subsubsec:htoinv_lepton_id_iso_reco_systs}

Electrons and photons are identified using the cut-based identification described in Chpts.~\ref{subsec:objects_electrons} and \ref{subsec:objects_photons}, respectively. Scale factors are applied separately to each of these objects in simulation that account for the efficiency of identification, isolation, and reconstruction in data. They are derived as a function of $\eta$ and \acrshort{ecal} supercluster \pt and binned as such. Uncertainties are given as the error in the $(\eta, \pt)$ bin of the histogram that describes the efficiencies. The ID and isolation are grouped as a single uncertainty, while reconstruction is another. Event weights simply are the products of the individual object scale factors. The uncertainty on the event weight is the product of the scale factor plus/minus its uncertainty. For each source, they are on the order of 1--5\,\%.

% While photon reconstruction efficiency is assumed to be 100\,\% for $H/E < \text{0.5}$, that is not the case for electrons. A tag and probe method is used to select electrons, distinguishing them from background objects. A gaussian is then fit to the invariant mass of the electron to capture the \PZ peak. 

% e/gamma reco SFs and systs documented here: https://twiki.cern.ch/twiki/bin/view/CMS/EgammaRunIIRecommendations#E_gamma_RECO

Muons follow a similar prescription, where the ID and isolation efficiencies are calculated in bins of $\eta$ and \pt. The systematic uncertainties for each of the two sources are separated---contrary to electrons and photons---and are usually 1\,\% or smaller in size. An equivalent reconstruction efficiency is not given as the dedicated muon chambers and reconstruction algorithms are more than sufficient.\footnote{For electrons, muons, and photons, not really sure of the actual procedures for getting the scale factors for each source. Twikis do a pretty poor job of explaining them.}

% Muon ID/ISO info: 2016 (https://twiki.cern.ch/twiki/bin/view/CMS/MuonReferenceEffs2016LegacyRereco), 2017 (https://twiki.cern.ch/twiki/bin/view/CMS/MuonReferenceEffs2017), 2018 (https://twiki.cern.ch/twiki/bin/view/CMS/MuonReferenceEffs2018)

Tau leptons are identified with a multivariate discriminator, as noted in Chpt.~\ref{subsec:objects_taus}. Data to simulation scale factors are applied and their uncertainties propagated through the analysis.\footnote{If we hard veto on taus, obviously won't have systs.}

% Tau ID and SF/uncertainty info: https://twiki.cern.ch/twiki/bin/viewauth/CMS/TauIDRecommendationForRun2#Corrections_for_the_MVA2017v2_ag


%=========================================================


\subsubsection{\texorpdfstring{\Pbottom-jets}{b-jets} tagged by the \texorpdfstring{\deepcsv}{DeepCSV} algorithm}
\label{subsubsec:htoinv_btagging_sfs}

Data--simulation scale factors are calculated for each \gls{bjet} as tagged by the \deepcsv algorithm. They are defined as the tagging efficiency in data divided by the efficiency for simulation for a given working point, and as a function of \gls{jet} \pt. Various methods are employed to derive separate scale factors for separate topologies, such as \acrshort{qcd} multijet or \ttbar events, and are described in Sec.~8.4 of Ref.~\citenum{Sirunyan:2017ezt}. A weighted average of the scale factors computed by each method are then taken to yield the final scale factor for a \Pbottom-tagged \gls{jet}. The resultant systematic uncertainty is typically less than 5\,\%.\footnote{We don't seem to take any explicit systematics at the skimming stage where our b-jets are defined and SFs taken. So I don't really know where the up and down variations come from.}

% This talk goes through the methods and SFs in more detail: https://cds.cern.ch/record/2627468/files/DP2018_033.pdf


%=========================================================


\subsubsection{Boosted jets tagged by the \texorpdfstring{\deepakeight}{DeepAK8} algorithm}
\label{subsubsec:htoinv_deepak8_sfs}

In a similar fashion to the section above, objects classified by the \deepakeight algorithm are given scale factors according to the data/\acrshort{mc} efficiencies as a function of AK8 \gls{jet} \pt. Scale factors are derived separately for \glspl{jet} tagged as originating from a top quark and \PVec boson. Systematic uncertainties associated with the scale factors on the order of 1--10\,\% are also derived separately, according to Sec.~8.1 of Ref.~\citenum{CMS-PAS-JME-18-002}.


%=========================================================


\subsection{Minor contributions}
\label{subsec:htoinv_minor_weights_systs}


%=========================================================


\subsubsection{Pre-firing in the ECAL}
\label{subsubsec:htoinv_ecal_prefiring_weight}

Derived by members of the Collaboration, the nominal weight applied to events in simulation that emulates the effect of pre-firing in 2016 and 2017 data, is given in Ref.~\citenum{prefiring_weight}:
\begin{equation}
    w_{\text{pre-firing}} = \prod_{i = \Pphoton,\, \mathrm{jet}} 1 - \epsilon_i^{\text{pre-firing}}(\eta, \pt^{\mathrm{EM}})
    \label{eq:prefiring_weight}
\end{equation}

where $\epsilon$ is the probability for an object to pre-fire as a function of its $\eta$ and electromagnetic \pt (simply \pt for photons, and \pt multiplied by the sum of the charged and neutral electromagnetic energy fractions for \glspl{jet}). The systematic uncertainty in the event weight is found by taking the uncertainty on $\epsilon$: the maximum of $\text{0.2}\epsilon$ and the statistical uncertainty in the $(\eta, \pt^{\mathrm{EM}})$ bin of the pre-firing map from which $\epsilon$ is extracted. Pre-firing probabilities are assumed to be uncorrelated between photons and jets.

% Pre-firing: https://twiki.cern.ch/twiki/bin/viewauth/CMS/L1ECALPrefiringWeightRecipe, https://indico.cern.ch/event/764279/


%=========================================================

\subsubsection{Jet energy scale and resolution}
\label{subsubsec:htoinv_JES_JER_systs}


%=========================================================


\subsubsection{Luminosity}
\label{subsubsec:htoinv_lumi_syst}

Uncertainties are associated with the various methods designed to measure the luminosity received by \acrshort{cms}. For 2016, 2017, and 2018, the total systematic uncertainties on certified data are 2.5\,\%, 2.3\,\%, and 2.5\,\%, respectively. They may be decomposed into several individual sources, some of which are correlated between years. These are documented in Tab.~\ref{tab:lumi_systs} with further information available in Refs.~\citenum{CMS-PAS-LUM-17-001}, \citenum{CMS-PAS-LUM-17-004}, and \citenum{CMS-PAS-LUM-18-002}.\footnote{Not sure if I even need the table, to be honest. It may ask more questions regarding each source rather than answering anything. Or, could fold into summary syst table.}

\begin{table}[htbp]
    \centering
    \begin{tabular}{lccc}
        \hline
        Uncertainty source & Impact in 2016 & Impact in 2017 & Impact in 2018 \\\hline
        Uncorrelated sources & 2.2 & 2.0 & 1.5 \\
        $x$-$y$ factorisation & 0.9 & 0.8 & 2.0 \\
        Length scale & --- & 0.3 & 0.2 \\
        Beam-beam deflection & 0.4 & 0.4 & --- \\
        Dynamic $\beta$ & 0.5 & 0.5 & --- \\
        Beam current calibration & --- & 0.3 & 0.2 \\
        Ghosts and satellites & 0.4 & 0.1 & ---  \\\hline
    \end{tabular}
    \caption[Sources of uncertainty in the luminosity measurements for each year of Run-2]{Sources of uncertainty in the luminosity measurements for each year of Run-2, and their impacts in percent. For each row except the first, the source of uncertainty is assumed to be correlated between years for which it was present.}
    \label{tab:lumi_systs}
\end{table}

% Lumi POG page: https://twiki.cern.ch/twiki/bin/view/CMS/TWikiLUM


%=========================================================


\subsubsection{Cross section for signal processes}
\label{subsubsec:htoinv_signal_xs_syst}

There are several sources of uncertainty in the computation of high order cross sections, from both theoretical and experimental sides. As with the nominal cross sections for signal processes, the uncertainties are retrieved from Ref.~\citenum{Cepeda:2019klc}. Every source for every signal process is treated as a separate uncertainty on the normalisation. They are correlated across years as the cross section is constant across them.


%=========================================================


\begin{table}[htbp]
    \centering
    \begin{tabular}{lcccc}
        \hline
        Source & Type & Years & Samples & Size (\%) \\\hline
        Pileup & Normalisation, shape & All, correlated & All & \\
        \acrshort{nlo} renorm. scale & Normalisation, shape & All, correlated & \acrshort{lo} $\PVec \plusjets$ & \\
        \acrshort{nlo} fact. scale & Normalisation, shape & All, correlated & \acrshort{lo} $\PVec \plusjets$ & \\
        \acrshort{nlo} PDF & Normalisation, shape & All, correlated & \acrshort{lo} $\PVec \plusjets$ & \\
        Trigger efficiency & Normalisation, shape & All, uncorrelated(?) & All & \\
        QCD renorm. \& fact. scale & Shape & All, correlated & \ttH, \ttbar, $\ttbar X$ & \\
        Top \pt reweighting & Shape & All, correlated & \ttbar & \\
        \Pmu ID efficiency & Normalisation & All, correlated & All & \\
        \Pmu isolation efficiency & Normalisation & All, correlated & All & \\
        \Pe ID \& isolation efficiency & Normalisation & All, correlated & All & \\
        \Pe reconstruction efficiency & Normalisation & All, correlated & All & \\
        \Pphoton ID \& isolation efficiency & Normalisation & All, correlated & All & \\
        \Ptau ID efficiency & Normalisation & All, correlated & All & \\
        \Pbottom-tagging & ? & All, correlated & All & \\
        Boosted object tagging & Normalisation & All, correlated & All & \\
        \acrshort{ecal} pre-firing & Normalisation & 2016--17, uncorrelated & All & \\
        \acrshort{jer} and \acrshort{jes} & Normalisation, shape(?) & All, correlated(?) & All & \\
        Luminosity & Normalisation & All & All & \\
        Photon purity & Normalisation & All, uncorrelated & \acrshort{qcd} est. from data & \\
        Signal cross section & Normalisation & All, correlated & Signal & \\\hline
    \end{tabular}
    \caption[The experimental uncertainties present in the analysis, the type of uncertainty, and impact on (the \ptmiss distribution/signal strength parameter), along with the years and simulated samples affected]{The experimental uncertainties present in the analysis, the type of uncertainty, and impact on (the \ptmiss distribution/signal strength parameter), along with the years and simulated samples affected.}
    \label{tab:htoinv_syst_summary}
\end{table}

% Could split source column into two (source, nuisance parameter/uncertainty) if I want to group things (e.g., luminosity source with x-y factorisation, length scale, beam-beam deflection, etc. nuisance parameters)
% Type would probably be shape/normalisation (or both)
% Years would be "all" or the specified years, noting correlations
% Samples/processes would be, e.g., all, or ttbar, LO V + jets, etc.
% Size would obviously depend on whether we're considering the effect on the MET distribution or the signal strength parameter. Would estimate in the signal region. And would have to decide whether the sizes are inclusive of category, or whether I have separate entries for each category (e.g., NLO corrections would presumably have larger impact in VH category)
